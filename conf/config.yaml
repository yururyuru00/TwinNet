key: SAGE_Reddit


# -------hydra & mlflow & optuna-------
# command: mlflow server --backend-store-uri sqlite:///tracking.db --default-artifact-root file:/tmp/artifacts --host 0.0.0.0

mlflow:
  server_ip: localhost # 192.168.11.30
  runname: test

defaults:
  - override hydra/sweeper: optuna # [basic, optuna]

hydra:
  sweeper:
    sampler:
      _target_: optuna.samplers.TPESampler
    direction: maximize
    n_jobs: 1
    n_trials: 1000


# -------datasets (planetoid)-------

Planetoid: &Planetoid
  task: transductive
  split: full # [public(semi-supervised), full(supervised)]
  n_tri: 10

Cora: &Cora
  <<: *Planetoid
  dataset: Cora
  x_normalize: False
  n_feat: 1433
  n_class: 7

CiteSeer: &CiteSeer
  <<: *Planetoid
  dataset: CiteSeer
  x_normalize: True
  n_feat: 3703
  n_class: 6

PubMed: &PubMed
  <<: *Planetoid
  dataset: PubMed
  x_normalize: False
  n_feat: 500
  n_class: 3


# -------datasets (web-kb)-------

WebKB: &WebKB
  task: transductive
  epochs: 5000 # override >> GNN
  x_normalize: False
  n_feat: 1703
  n_tri: 10 # following original paper
  n_class: 5

Cornell: &Cornell
  <<: *WebKB
  dataset: Cornell

Texas: &Texas
  <<: *WebKB
  dataset: Texas

Wisconsin: &Wisconsin
  <<: *WebKB
  dataset: Wisconsin


# -------datasets (others)-------

PPI: &PPI
  dataset: PPI
  task: transductive
  n_feat: 1 # num. of node feat
  e_feat: 8 # num. of edge feat
  n_class: 112
  n_tri: 3
  epochs: 1000 # override >> GNN

Arxiv: &Arxiv
  dataset: Arxiv
  task: transductive
  n_feat: 128
  n_class: 40
  n_tri: 5
  epochs: 500 # override >> GNN

Reddit: &Reddit
  dataset: Reddit
  task: transductive
  n_feat: 602
  n_class: 41
  n_tri: 3
  epochs: 11 # override >> GNN

PPIinduct: &PPIinduct
  dataset: PPIinduct
  task: inductive
  n_feat: 50
  n_class: 121
  n_tri: 5


# -------model interface-------

GNN: &GNN
  base_gnn: GNN
  global_skip_connection: vanilla # [vanilla, jk, twin]
  skip_connection: vanilla # [vanilla, res, dense, highway]
  n_layer: 3
  n_hid: 16
  dropout: 0.4
  learning_rate: 0.001
  weight_decay: 0.0001
  seed: 42
  epochs: 200
  patience: 100
  norm: None # [None, LayerNorm, BatchNorm1d]


# -------model instance-------

# Graph Convolution Networks (GCN)
GCN: &GCN
  <<: *GNN
  activation: ReLU
  base_gnn: GCN # override >> GNN

# Graph Attention Networks (GAT)
GAT: &GAT
  <<: *GNN
  activation: ELU
  base_gnn: GAT # override >> GNN
  dropout_att: 0.6
  n_head: 8
  n_head_last: 1

# Graph SAGE (SAGE)
SAGE: &SAGE
  <<: *GNN
  activation: ReLU
  base_gnn: SAGE # override >> GNN


# -------model instance + dataset-------

GCN_Cora: &GCN_Cora
  <<: [*Cora, *GCN]
  # not yet tuning

GAT_Cora: &GAT_Cora
  <<: [*Cora, *GAT]
  # not yet tuning

GCN_CiteSeer: &GCN_CiteSeer
  <<: [*CiteSeer, *GCN]
  # not yet tuning

GAT_CiteSeer: &GAT_CiteSeer
  <<: [*CiteSeer, *GAT]
  # not yet tuning

GAT_PubMed: &GAT_PubMed
  <<: [*PubMed, *GAT]
  norm: BatchNorm1d
  n_layer: 2
  n_hid: 16
  n_head_last: 8
  dropout: 0.2
  learning_rate: 0.001
  weight_decay: 0.001

GAT_PPIinduct: &GAT_PPIinduct
  <<: [*PPIinduct, *GAT]
  skip_connection: highway
  norm: BatchNorm1d
  n_layer: 3
  n_hid: 256
  n_head: 4
  n_head_last: 6
  dropout: 0
  dropout_att: 0
  learning_rate: 0.001
  weight_decay: 0

GCN_Arxiv: &GCN_Arxiv
  <<: [*Arxiv, *GCN]
  norm: BatchNorm1d
  n_layer: 5
  n_hid: 256
  dropout: 0.5
  learning_rate: 0.001
  weight_decay: 0

SAGE_Reddit: &SAGE_Reddit
  <<: [*Reddit, *SAGE]
  norm: None
  n_layer: 2
  n_hid: 128
  dropout: 0
  learning_rate: 0.001
  weight_decay: 0

SAGE_PPI: &SAGE_PPI
  <<: [*PPI, *SAGE]
  norm: BatchNorm1d
  n_layer: 4
  n_hid: 256
  dropout: 0.6
  learning_rate: 0.005
  weight_decay: 0



# Jumping Knowledge Networks (JKNet)
JK: &JK
  global_skip_connection: jk # [vanilla, jk, twin]
  jk_mode: cat # [cat, max, lstm]


JKGCN_Cora: &JKGCN_Cora
  <<: [*JK, *GCN_Cora]

JKGAT_Cora: &JKGAT_Cora
  <<: [*JK, *GAT_Cora]

JKGCN_CiteSeer: &JKGCN_CiteSeer
  <<: [*JK, *GCN_CiteSeer]

JKGAT_CiteSeer: &JKGAT_CiteSeer
  <<: [*JK, *GAT_CiteSeer]

JKGAT_PubMed: &JKGAT_PubMed
  <<: [*JK, *GAT_PubMed]

JKGAT_PPIinduct: &JKGAT_PPIinduct
  <<: [*JK, *GAT_PPIinduct]

JKGCN_Arxiv: &JKGCN_Arxiv
  <<: [*JK, *GCN_Arxiv]

JKSAGE_Reddit: &JKSAGE_Reddit
  <<: [*JK, *SAGE_Reddit]

JKSAGE_PPI: &JKSAGE_PPI
  <<: [*JK, *SAGE_PPI]



# -------model instance (our study)-------

# Twin-Graph Neural Networks (Twin-GNN)
Twin: &Twin
  global_skip_connection: twin # [vanilla, jk, twin]
  temparature: 1 # \in R
  scope: local # [local, global]
  kernel: sdp # [dp, sdp, wdp, ad, mx]
  self_loop: True # [True, False]


TwinGCN_Cora: &TwinGCN_Cora
  <<: [*Twin, *GCN_Cora]
  # not yet tuning

TwinGAT_Cora: &TwinGAT_Cora
  <<: [*Twin, *GAT_Cora]
  # not yet tuning

TwinGCN_CiteSeer: &TwinGCN_CiteSeer
  <<: [*Twin, *GCN_CiteSeer]
  # not yet tuning

TwinGAT_CiteSeer: &TwinGAT_CiteSeer
  <<: [*Twin, *GAT_CiteSeer]
  # not yet tuning

TwinGAT_PubMed: &TwinGAT_PubMed
  <<: [*Twin, *GAT_PubMed]
  n_layer: 6
  kernel: mx
  temparature: -0.7

TwinGAT_PPIinduct: &TwinGAT_PPIinduct
  <<: [*Twin, *GAT_PPIinduct]
  n_layer: 6
  kernel: mx # maybe sdp

TwinGCN_Arxiv: &TwinGCN_Arxiv
  <<: [*Twin, *GCN_Arxiv]
  n_layer: 6
  kernel: mx
  temparature: -0.7

TwinSAGE_Reddit: &TwinSAGE_Reddit
  <<: [*Twin, *SAGE_Reddit]
  # not yet tuning

TwinSAGE_PPI: &TwinSAGE_PPI
  <<: [*Twin, *SAGE_PPI]
  n_layer: 8
  kernel: wdp # maybe sdp
  temparature: 0.5